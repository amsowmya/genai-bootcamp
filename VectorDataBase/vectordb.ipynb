{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_loader = PyPDFDirectoryLoader(\"E:\\\\2025\\\\Generative_AI\\\\Course\\\\genai-bootcamp\\\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = directory_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"E:\\\\2025\\\\Generative_AI\\\\Course\\\\genai-bootcamp\\\\data\\\\llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗Louis Martin†Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'E:\\\\2025\\\\Generative_AI\\\\Course\\\\genai-bootcamp\\\\data\\\\llama2.pdf',\n",
       " 'page': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'E:\\\\2025\\\\Generative_AI\\\\Course\\\\genai-bootcamp\\\\data\\\\llama2.pdf', 'page': 0}, page_content='Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗Louis Martin†Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    azure_deployment=os.getenv('AZURE_EMBEDDING_DEPLOYMENT_MODEL'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗Louis Martin†Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(openai_embedding.embed_query(documents[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = faiss.IndexFlatIP(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS(\n",
    "    embedding_function=openai_embedding,\n",
    "    index=faiss_index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['596c72c2-15e0-44d8-be22-e349e5c3a5da',\n",
       " '2393efb5-d3f9-4342-8c9a-9708782f097e',\n",
       " '9d6a0b1e-4634-4ed5-bf72-5b214c3f9339',\n",
       " '279a6bae-331e-40c6-affb-8f74402f5521',\n",
       " 'c5d8bbb4-9815-4f5a-8445-bad0deaeb9fa',\n",
       " '5e7c50d7-b32c-4389-a6ed-02e9164936f2',\n",
       " '43d932da-25c8-47e4-ac5e-b5bfa3fd7428',\n",
       " '9787a9db-cd4b-4242-9677-782b633d750f',\n",
       " '4e7302c7-05b1-4057-b0b6-b08e70c7e4a6',\n",
       " 'bd974558-fcaa-41d4-aa9d-2a77f34ce5d3',\n",
       " '6adc825a-d938-4865-8c4f-97e6c52e62a6',\n",
       " 'b99422fe-b4b4-4303-9867-d61c37372b86',\n",
       " 'e40aa928-4f76-47af-baf7-7d86b47a51ec',\n",
       " 'bf469859-65c3-4836-9dfe-9ca9d456e678',\n",
       " '41b2df04-5e36-43cf-9b4d-66088f361281',\n",
       " 'f7f6403e-82e5-4429-99da-c3e389d4377f',\n",
       " '8d4c366d-2c0f-4ef8-8b29-5c68795402e7',\n",
       " '379a3353-3dc3-4448-ac6f-7d3226ad230b',\n",
       " '58833967-9dad-465f-98fc-1b9bb91a2db5',\n",
       " '91f9c889-403e-478e-b960-86ccf3b6bf5c',\n",
       " '76d64237-e484-43d4-a5bc-9ceccb4b439f',\n",
       " '34e3b288-cbed-4b75-97d5-e6596cd1d3ea',\n",
       " '60af069e-1cdd-4ffc-bb6d-2cd989fc95a6',\n",
       " '3da46343-4833-4783-87ba-63125cf540a2',\n",
       " 'cb20d95f-ca7e-4e5b-9d27-65169bc9f450',\n",
       " 'cf56274d-fdbf-4f28-a886-f5e7e8959488',\n",
       " '7e85beb4-0fdc-42f1-a1ff-890eff611ce9',\n",
       " '04caf326-c9e7-4b37-8656-43dcd9ffeed6',\n",
       " 'cb8b1c2e-2d69-4c78-9e3b-931a41ff2c80',\n",
       " 'e80e08b3-7865-41a7-9f61-a4f14b31a52b',\n",
       " '3b9d7421-ca97-487e-bc4c-1a1ed1743445',\n",
       " 'a08615d9-d53d-4c1a-9ff8-a182d433b271',\n",
       " 'ce7282dd-e504-4440-82d1-4c7cf5aba089',\n",
       " '5cf15bf4-d5f4-40b6-8cf5-3f6045167977',\n",
       " '697578f6-b841-45c5-8775-f61354d8515c',\n",
       " '40366992-8137-4ab8-8807-226cfc08ef81',\n",
       " 'd88eb49c-6267-46c9-b947-8a39ff3015f4',\n",
       " '6b83ecc0-a6c2-4c5f-b6e1-916f17fbb87b',\n",
       " '26d3cdb6-fdd0-47f5-8533-8f0d62e13e7d',\n",
       " '819dd4bc-7b66-46b0-b7cb-207609121dba',\n",
       " 'c99efb9f-570c-4e70-8500-0920a43b0f0c',\n",
       " '7dade70f-a011-428c-9987-2ac8a4c52651',\n",
       " '3aa62219-a7c5-4266-aa14-8d39b445c721',\n",
       " '4916045e-befe-4cec-8af7-259c58faac27',\n",
       " '6341b4fe-d766-470d-b44a-abc5c1229342',\n",
       " 'd029ff60-49ee-4df4-99b1-23e837d0192f',\n",
       " '1a102cb3-d041-4dde-af31-13f2fd5911be',\n",
       " 'b0c3dcc7-144a-4385-947d-ab9b1b33b40b',\n",
       " '825fe71d-c617-474c-a8e7-c2e0b62899aa',\n",
       " '684b20d8-7b3d-4cfa-b113-493b0b65622c',\n",
       " '0f234650-c032-4f9d-83c8-63727449c980',\n",
       " 'eb8d9ba1-be34-441d-8397-73e5b2358e16',\n",
       " 'b24ba5da-ff2e-46bc-bad6-dc254e0d23bf',\n",
       " 'd29dd633-166d-4d12-99c0-eede7b31b1b3',\n",
       " 'a5bd98e1-83ed-453a-b13a-aff043155aeb',\n",
       " 'dbb7a0cb-8249-4b60-ad91-8d29fe3eb382',\n",
       " 'c59ff727-88f6-419e-8503-cda6cb36a837',\n",
       " '4dfcf882-582f-4681-9243-bf13301eab8c',\n",
       " '34ba7fe8-3dbf-447e-a16a-0229c65bb86b',\n",
       " '4ae8d16d-79a4-4a40-8594-1ea3c3f1c1bc',\n",
       " 'cebed915-7221-4be4-9b41-d08a3fe1cd80',\n",
       " 'daaaf962-ce81-431f-9e2d-018a0d0dc18f',\n",
       " 'e3161f74-b640-49df-b75a-092b6cbfb21c',\n",
       " '58d5c584-a0a3-4fdd-9777-d46b9833c41f',\n",
       " 'adda7576-cc56-4eb9-a422-6a12d919463a',\n",
       " '35358520-dd77-455d-82d6-11d1f5c369fd',\n",
       " '9b72ad92-3b0c-4a0a-9736-7b9614480f62',\n",
       " 'de807129-cc79-4e81-976f-2a458d5bd618',\n",
       " '300154e1-573a-4199-8179-c0ef7c0ccc32',\n",
       " '6bbedb87-869d-49a1-b969-e4059ca4d6f3',\n",
       " '29ce8d5f-3b16-4e51-8c44-b7c69a770296',\n",
       " '900b1c85-18a0-454a-af7d-f212811a6334',\n",
       " '85047e90-f9ea-4880-9a4b-ffd50854f11f',\n",
       " 'a8ec2cf4-1cd4-49e7-9bdf-bb13f624a091',\n",
       " '790e782c-f628-4ade-a469-d0f70b6917bf',\n",
       " '586beb05-bfc3-420d-bc06-713df42ed195',\n",
       " '14116d7c-2d8b-4111-86dd-26f452161c41']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c5d8bbb4-9815-4f5a-8445-bad0deaeb9fa', metadata={'source': 'E:\\\\2025\\\\Generative_AI\\\\Course\\\\genai-bootcamp\\\\data\\\\llama2.pdf', 'page': 4}, page_content='Figure4: Trainingof Llama 2-Chat : Thisprocessbeginswiththe pretraining ofLlama 2 usingpublicly\\navailableonlinesources. Followingthis,wecreateaninitialversionof Llama 2-Chat throughtheapplication\\nofsupervised fine-tuning . Subsequently, the model is iteratively refined using Reinforcement Learning\\nwith Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy\\nOptimization(PPO).ThroughouttheRLHFstage,theaccumulationof iterativerewardmodelingdata in\\nparallel with model enhancements is crucial to ensure the reward models remain within distribution.\\n2 Pretraining\\nTocreatethenewfamilyof Llama 2models,webeganwiththepretrainingapproachdescribedinTouvronetal.\\n(2023), using an optimized auto-regressive transformer, but made several changes to improve performance.\\nSpecifically,weperformedmorerobustdatacleaning,updatedourdatamixes,trainedon40%moretotal\\ntokens,doubledthecontextlength,andusedgrouped-queryattention(GQA)toimproveinferencescalability\\nfor our larger models. Table 1 compares the attributes of the new Llama 2 models with the Llama 1 models.\\n2.1 Pretraining Data\\nOur training corpus includes a new mix of data from publicly available sources, which does not include data\\nfromMeta’sproductsorservices. Wemadeanefforttoremovedatafromcertainsitesknowntocontaina\\nhighvolumeofpersonalinformationaboutprivateindividuals. Wetrainedon2trilliontokensofdataasthis\\nprovidesagoodperformance–costtrade-off,up-samplingthemostfactualsourcesinanefforttoincrease\\nknowledge and dampen hallucinations.\\nWeperformedavarietyofpretrainingdatainvestigationssothatuserscanbetterunderstandthepotential\\ncapabilities and limitations of our models; results can be found in Section 4.1.\\n2.2 Training Details\\nWe adopt most of the pretraining setting and model architecture from Llama 1 . We use the standard\\ntransformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and\\nSennrich, 2019), use the SwiGLU activation function (Shazeer, 2020), and rotary positional embeddings\\n(RoPE, Su et al. 2022). The primary architectural differences from Llama 1 include increased context length\\nandgrouped-queryattention(GQA).WedetailinAppendixSectionA.2.1eachofthesedifferenceswith\\nablation experiments to demonstrate their importance.\\nHyperparameters. We trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with β1=\\n0.9, β2= 0.95,eps= 10−5. We use a cosine learning rate schedule, with warmup of 2000 steps, and decay\\nfinallearningratedownto10%ofthepeaklearningrate. Weuseaweightdecayof 0.1andgradientclipping\\nof1.0. Figure 5 (a) shows the training loss for Llama 2 with these hyperparameters.\\n5')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\n",
    "    query=\"what is llama2 and what is the difference between llama2 and mistral?\",\n",
    "    k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
